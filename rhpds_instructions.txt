1. Machine sensor pods are crash-looping (Stormshift). Fix:
    in project `manuela-stormshift-machine-sensor`
    in configmaps `machine-sensor-1` and `machine-sensor-2`
    in value of key `MQTT_HOSTNAME`
    replace `edge` by `cluster` (factory cluster reference)

2. Inference pod unable to pull image. Fix:
    in project
    in `Seldon Operator`
    in SeldonDeployment `anomaly-detection`
    in spec.predictors[0].componentSpecs[0].spec.containers[0].image
    replace `manuela-tst-all` by `manuela-stormshift-messaging` (image stream reference)

3. Kafka-to-S3 integration pods unable to start. Fix:
    increase worker node count from 2 to 3
    install OpenShift Container Storage operator
    in projects `factory-s3-store` and `manuela-data-lake-central-s3-store`
    provide S3 credentials within secret `s3-secret`

4. JupyterHub pods not running. Fix:
    in project `manuela-ml-workspace`
    delete imagestream `jupyterhub`

    in file `industrial-edge/charts/datacenter/opendatahub/templates/odh-kfdef.yaml`
    udpate spec.repos:
    - name: kf-manifests
      uri: 'https://github.com/opendatahub-io/manifests/tarball/v1.4.0-rc.2-openshift'
    - name: manifests
      uri: 'https://github.com/opendatahub-io/odh-manifests/tarball/v1.1.2'
    push changes

5. Anomaly detection not running. Fix:
    in file `industrial-edge/charts/datacenter/manuela-tst/values.yaml`
    update subscriptions.version.seldon:
      v1.12.0
    push changes

6. Sensor data not shown in line dashboard (Stormshift). Fix:
    in project `manuela-stormshift-line-dashboard`
    in configmap `line-dashboard-configmap`
    in data.config.json
    in value of key `websocketHost`
    replace `edge` by `cluster` (factory cluster reference)



Get ArgoCD credentials

ARGO_CMD=`oc get secrets -A -o jsonpath='{range .items[*]}{"oc get -n "}{.metadata.namespace}{" routes; oc -n "}{.metadata.namespace}{" extract secrets/"}{.metadata.name}{" --to=-\\n"}{end}' | grep gitops-cluster`
CMD=`echo $ARGO_CMD | sed 's|- oc|-;oc|g'`
eval $CMD



Set up MLOps scenario

1. Set up ODF

2. set environment variables when provisioning notebook (optional):
    S3_ACCESS_KEY
    S3_SECRET_KEY
    GIT_USER
    GIT_PASSWORD

3. set Runtime image in Elyra workspace
    generic-datascience
    quay.io/thoth-station/s2i-generic-data-science-notebook

4. ensure runtime image set in demo.pipeline

5. set up Elyra runtime
    http://ml-pipeline-ui.kubeflow.svc.cluster.local (or external)
    Tekton pipeline engine
    http://s3.openshift-storage.svc.cluster.local/ (or external)
    S3 credentials
    S3 bucket name

6. clone ops repo

7. update anomaly-detection manifests

8. push changes

9. deploy seldon-rclone-secret.yaml

10. ensure TARGET REVISION is set to "main" in ArgoCD application